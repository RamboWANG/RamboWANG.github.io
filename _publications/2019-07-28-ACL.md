---
title: "Bayes Test of Precision, Recall, and F$_1$ Measure for Comparison of Two Natural Language Processing Models"
collection: publications
permalink: /publication/2019-07-28-ACL
excerpt: 'Direct comparison on point estimation of the precision (P), recall (R), and F$_1$ measure of two natural language processing (NLP) models on a common test corpus is unreasonable and results in less replicable conclusions due to a lack of a statistical test. However, the existing $t$-tests in cross-validation (CV) for model comparison are inappropriate because the distributions of P, R, F$_1$ are skewed and an interval estimation of P, R, and F$_1$ based on a $t$-test may exceed [0,1].  In this study, we propose to use a block-regularized $3\times 2$ CV ($3\times 2$ BCV) in model comparison because it could regularize the difference in certain frequency distributions over linguistic units between training and validation sets and yield stable estimators of P, R, and F$_1$.  On the basis of the $3\times 2$ BCV, we calibrate the posterior distributions of P, R, and F$_1$ and derive an accurate interval estimation of P, R, and F$_1$. Furthermore, we formulate the comparison into a hypothesis testing problem and propose a novel Bayes test. The test could directly compute the probabilities of the hypotheses on the basis of the posterior distributions and provide more informative decisions than the existing significance $t$-tests. Three experiments with regard to NLP chunking tasks are conducted, and the results illustrate the validity of the Bayes test.'
date: 2019-10-24
venue: 'ACL 2019'
paperurl: 'https://www.sciencedirect.com/science/article/pii/S0020025518308582'
citation: 'Ruibo Wang, Jihong Li, Xingli Yang, Jing Yang. (2018). &quot;Block-regularized repeated learning-testing for estimating generalization error.&quot; <i>ACL 2019</i>.'
---

Direct comparison on point estimation of the precision (P), recall (R), and F$_1$ measure of two natural language processing (NLP) models on a common test corpus is unreasonable and results in less replicable conclusions due to a lack of a statistical test. However, the existing $t$-tests in cross-validation (CV) for model comparison are inappropriate because the distributions of P, R, F$_1$ are skewed and an interval estimation of P, R, and F$_1$ based on a $t$-test may exceed [0,1].  In this study, we propose to use a block-regularized $3\times 2$ CV ($3\times 2$ BCV) in model comparison because it could regularize the difference in certain frequency distributions over linguistic units between training and validation sets and yield stable estimators of P, R, and F$_1$.  On the basis of the $3\times 2$ BCV, we calibrate the posterior distributions of P, R, and F$_1$ and derive an accurate interval estimation of P, R, and F$_1$. Furthermore, we formulate the comparison into a hypothesis testing problem and propose a novel Bayes test. The test could directly compute the probabilities of the hypotheses on the basis of the posterior distributions and provide more informative decisions than the existing significance $t$-tests. Three experiments with regard to NLP chunking tasks are conducted, and the results illustrate the validity of the Bayes test.

[Download paper here](https://rambowang.github.io/files/200622402008_ms_thesis_wangrb.pdf)

Recommended citation: Ruibo Wang, Jihong Li. (2018). &quot;Bayes Test of Precision, Recall, and F$_1$ Measure for Comparison of Two Natural Language Processing Models.&quot; <i>ACL 2019</i>. 



























